{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('model/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing raw reddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cephcyn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cephcyn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# print(lemmatizer.lemmatize(\"cats\"))\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def parse_reddit_csv(filename):\n",
    "    print(\"Reading from\", filename)\n",
    "    csv_cols = []\n",
    "    frequencies = {}\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            # Remove numbers, punctuation\n",
    "            row['selftext'] = re.sub(r'\\d+', '', row['selftext'])\n",
    "            row['title'] = re.sub(r'\\d+', '', row['title'])\n",
    "            # Tokenize the post text (selftext) and post title\n",
    "            post_tokens = tokenizer.tokenize(row['selftext'])\n",
    "            title_tokens = tokenizer.tokenize(row['title'])\n",
    "            # Filter out stopwords\n",
    "            post_tokens = [w for w in post_tokens if not w in stop_words]\n",
    "            title_tokens = [w for w in title_tokens if not w in stop_words]\n",
    "            # Lemmatize the post text (reduce words to word stems i.e. cats->cat, liked->like)\n",
    "            post_tokens = [lemmatizer.lemmatize(w, 'n') for w in post_tokens]\n",
    "            post_tokens = [lemmatizer.lemmatize(w, 'v') for w in post_tokens]\n",
    "            title_tokens = [lemmatizer.lemmatize(w, 'n') for w in title_tokens]\n",
    "            title_tokens = [lemmatizer.lemmatize(w, 'v') for w in title_tokens]\n",
    "            csv_cols.append({'author': row['author'],\n",
    "                             'selftext': post_tokens,\n",
    "                             'title': title_tokens,\n",
    "                             'post_id': row['id']})\n",
    "            # TODO need to collect frequencies of words in the entire corpus\n",
    "            # TODO update frequencies mapping from word->count and also get a sum\n",
    "    return csv_cols, frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from data/final_proj_data_preprocessed_1000sample.csv\n"
     ]
    }
   ],
   "source": [
    "parsed, corpus_freq = parse_reddit_csv('data/final_proj_data_preprocessed_1000sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate post embeddings (Word2Vec)\n",
    "(using selftext only)\n",
    "\n",
    "The first (thereafter called W2VWeighted) is calculated by weighing the contribution of each word embedding by the inverse of its relative frequency to the final sentence embedding.\n",
    "\n",
    "In doing so, the contributions of the most common words are minimized.\n",
    "\n",
    "The second (thereafter called W2V-SIF) is calculated by first taking the weighed sentence embedding before removing the first principal component from it.\n",
    "\n",
    "Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\\\n",
    "A simple but tough-to-beat baseline for sentence embeddings. In ICLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_emb = {}\n",
    "for i in range(len(parsed[0])):\n",
    "    counts = Counter(parsed[0][i]['selftext']).items()\n",
    "    freq = pd.DataFrame(counts)\n",
    "    freq = freq.rename(columns={0: \"word\", 1: 'freq'})\n",
    "    # Weight by inverse relative frequency\n",
    "    freq['inv_rfreq'] = freq['freq'].sum()/freq['freq']\n",
    "    unknowns = []\n",
    "    emb_dict = {}\n",
    "    for w in freq['word'].to_list():\n",
    "        try:\n",
    "            emb = model[w]\n",
    "            emb_dict.update({w:emb})\n",
    "        except:\n",
    "            unknowns.append(w)\n",
    "    emb_value = pd.DataFrame(emb_dict).transpose().reset_index()\n",
    "    emb_value = emb_value.rename(columns={'index': \"word\"})\n",
    "    emb_value_list = emb_value.iloc[:, 1:301].mul(freq['inv_rfreq'], axis = 0).sum().to_list()\n",
    "    sen_emb.update({parsed[0][i]['post_id']:emb_value_list})       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample1000_emb.pickle', 'wb') as handle:\n",
    "    pickle.dump(sen_emb, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate post topics (LDA)\n",
    "(using both selftext and title)\n",
    "\n",
    "A Bag of Words (BoW) corpus was obtained before a term frequency-inverse document frequency (TF-IDF) corpus was derived from it.\n",
    "\n",
    "Topic modeling was then performed on both the BoW corpus (thereafter LDA-BoW) and\n",
    "TF-IDF corpus (thereafter LDA-TFIDF) with the number of topics set to 30, in line with the number of clusters used.\n",
    "\n",
    "The document-topic mapping of each post is then used for computing cosine similarities with all other posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(8311 unique tokens: ['Best', 'But', 'Hello', 'I', 'Unfortunately']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary([parsed[i]['selftext'] for i in range(len(parsed))])\n",
    "print(dictionary)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(parsed[i]['selftext']) for i in range(len(parsed))]\n",
    "# for doc in corpus:\n",
    "#     print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "\n",
    "# TF-IDF (term freq, inverse document freq) representation\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "# for doc in tfidf[corpus]:\n",
    "#     print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "def get_topics(corpus):\n",
    "    # Train LDA model, get model & topic vectors\n",
    "    # Set training parameters.\n",
    "    num_topics = 30\n",
    "    chunksize = 100\n",
    "    passes = 20\n",
    "    iterations = 400\n",
    "    eval_every = 100  # None = Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make a index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "    \n",
    "    # Get basic evaluation\n",
    "    top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "    # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "    print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "    \n",
    "    # Get topic vectors\n",
    "    all_topics = model.get_document_topics(corpus, per_word_topics=True)\n",
    "    all_topics = [(doc_topics, word_topics, word_phis) for doc_topics, word_topics, word_phis in all_topics]\n",
    "    sen_top = {}\n",
    "    for i in range(len(parsed)):\n",
    "        # These are in the same order as the documents themselves.\n",
    "        doc_topics, word_topics, phi_values = all_topics[i]\n",
    "        sen_top.update({parsed[i]['post_id']:doc_topics})\n",
    "    \n",
    "    return model, sen_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -13.8101.\n"
     ]
    }
   ],
   "source": [
    "# Get bow data\n",
    "model_bow, sen_top_bow = get_topics(corpus)\n",
    "\n",
    "with open('sample1000_top_bow_model.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_bow, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('sample1000_top_bow.pickle', 'wb') as handle:\n",
    "    pickle.dump(sen_top_bow, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -13.0959.\n"
     ]
    }
   ],
   "source": [
    "# Get bow data\n",
    "model_tfidf, sen_top_tfidf = get_topics(tfidf[corpus])\n",
    "\n",
    "with open('sample1000_top_tfidf_model.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('sample1000_top_tfidf.pickle', 'wb') as handle:\n",
    "    pickle.dump(sen_top_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Pairwise Cosine Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_emb_arr = np.array(list(sen_emb.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(sen_emb_arr,sen_emb_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
