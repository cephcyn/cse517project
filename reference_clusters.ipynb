{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import models\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "test_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"csv_file_name\", required=True, help=\"Filename of the Reddit-scrape-data CSV we are running ref models on\")\n",
    "# parser.add_argument(\"setname\", required=True, help=\"Title of this experiment\")\n",
    "# parser.add_argument(\"num_clusters\", type=int, default=6, required=False, help=\"Number of clusters (paper used 30)\")\n",
    "# args = parser.parse_args()\n",
    "# print(args.csv_file_name)\n",
    "# print(args.setname)\n",
    "\n",
    "# setname = args.setname\n",
    "# src_file = args.csv_file_name\n",
    "# num_clusters = num_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing raw reddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# print(lemmatizer.lemmatize(\"cats\"))\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def parse_reddit_csv(filename, setname, stop_words=None, lemmatizer=None, tokenizer=None):\n",
    "    if lemmatizer == None:\n",
    "        nltk.download('wordnet')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    if stop_words == None or tokenizer == None:\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    print(f'parse_reddit_csv({filename}, {setname})')\n",
    "    print('START:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    t0 = time.process_time()\n",
    "    print(\"Reading from\", filename)\n",
    "    csv_cols = []\n",
    "    authors = {}\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            # Remove numbers, punctuation\n",
    "            row['selftext'] = re.sub(r'\\d+', '', row['selftext'])\n",
    "            row['title'] = re.sub(r'\\d+', '', row['title'])\n",
    "            # Tokenize the post text (selftext) and post title\n",
    "            post_tokens = tokenizer.tokenize(row['selftext'])\n",
    "            title_tokens = tokenizer.tokenize(row['title'])\n",
    "            # Filter out stopwords\n",
    "            post_tokens = [w for w in post_tokens if not w in stop_words]\n",
    "            title_tokens = [w for w in title_tokens if not w in stop_words]\n",
    "            # Lemmatize the post text (reduce words to word stems i.e. cats->cat, liked->like)\n",
    "            post_tokens = [lemmatizer.lemmatize(w, 'n') for w in post_tokens]\n",
    "            post_tokens = [lemmatizer.lemmatize(w, 'v') for w in post_tokens]\n",
    "            title_tokens = [lemmatizer.lemmatize(w, 'n') for w in title_tokens]\n",
    "            title_tokens = [lemmatizer.lemmatize(w, 'v') for w in title_tokens]\n",
    "            csv_cols.append({'author': row['author'],\n",
    "                             'selftext': post_tokens,\n",
    "                             'title': title_tokens,\n",
    "                             'post_id': row['id']})\n",
    "            # Add author mapping\n",
    "            if row['author'] not in authors:\n",
    "                authors[row['author']] = []\n",
    "            authors[row['author']].append(row['id'])\n",
    "    print('PROCESS TIME ELAPSED (s)', time.process_time() - t0)\n",
    "    with open(f'partials/{setname}_parse.pickle', 'wb') as handle:\n",
    "        pickle.dump(csv_cols, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f'partials/{setname}_parse_authors.pickle', 'wb') as handle:\n",
    "        pickle.dump(authors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('parse_reddit_csv ENDED:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    parse_reddit_csv('data/final_proj_data_preprocessed_1000sample.csv', 'sample1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate post embeddings (Word2Vec)\n",
    "(using selftext only)\n",
    "\n",
    "The first (thereafter called W2VWeighted) is calculated by weighing the contribution of each word embedding by the inverse of its relative frequency to the final sentence embedding.\n",
    "\n",
    "In doing so, the contributions of the most common words are minimized.\n",
    "\n",
    "The second (thereafter called W2V-SIF) is calculated by first taking the weighed sentence embedding before removing the first principal component from it.\n",
    "\n",
    "Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\\\n",
    "A simple but tough-to-beat baseline for sentence embeddings. In ICLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def embed_w2v(setname, model=None):\n",
    "    # Load the parse\n",
    "    with open(f'partials/{setname}_parse.pickle', 'rb') as handle:\n",
    "        parsed = pickle.load(handle)\n",
    "    \n",
    "    # Load Google's pre-trained Word2Vec model.\n",
    "    if model == None:\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format('model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    \n",
    "    print(f'embed_w2v({setname})')\n",
    "    print('START:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    t0 = time.process_time()\n",
    "    # Build weighted embeddings\n",
    "    weighted_emb = {}\n",
    "    for i in range(len(parsed)):\n",
    "        counts = Counter(parsed[i]['selftext'])\n",
    "        freq = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "        freq = freq.rename(columns={'index': 'word', 0: 'freq'})\n",
    "        # Weight by inverse relative frequency\n",
    "        freq['inv_rfreq'] = freq['freq'].sum()/freq['freq']\n",
    "        unknowns = []\n",
    "        emb_dict = {}\n",
    "        for w in freq['word']:\n",
    "            try:\n",
    "                emb = model[w]\n",
    "                emb_dict.update({w:emb})\n",
    "            except:\n",
    "                unknowns.append(w)\n",
    "        emb_value = pd.DataFrame(emb_dict).transpose().reset_index()\n",
    "        emb_value = emb_value.rename(columns={'index': 'word'})\n",
    "        emb_value_list = list(emb_value.iloc[:, 1:301].mul(freq['inv_rfreq'], axis = 0).sum())\n",
    "        weighted_emb.update({parsed[i]['post_id']:emb_value_list})\n",
    "    # Build SIF (remove first principal component)\n",
    "    pca = PCA()\n",
    "    ids = [key for (key, val) in list(weighted_emb.items())]\n",
    "    weighted_matrix = np.array([val for (key, val) in list(weighted_emb.items())])\n",
    "    # calculate PCA projections\n",
    "    pca_matrix = pca.fit_transform(weighted_matrix)\n",
    "    # calculate p-component that we need to subtract\n",
    "    pca_adjust = [[emb[0] * c for c in pca.components_[0]] for emb in pca_matrix.tolist()]\n",
    "    # drop p-component\n",
    "    sif_matrix = [[i - j for i, j in zip(emb, pc)] for emb, pc in zip(weighted_matrix.tolist(), pca_adjust)]\n",
    "    # convert back to dict format\n",
    "    sif_emb = dict(zip(ids, sif_matrix))\n",
    "    print('PROCESS TIME ELAPSED (s)', time.process_time() - t0)\n",
    "    with open(f'partials/{setname}_embed_w2v_weighted.pickle', 'wb') as handle:\n",
    "        pickle.dump(weighted_emb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f'partials/{setname}_embed_w2v_sif.pickle', 'wb') as handle:\n",
    "        pickle.dump(sif_emb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('embed_w2v ENDED:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    embed_w2v('sample1000', model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate post topics (LDA)\n",
    "(using both selftext and title)\n",
    "\n",
    "A Bag of Words (BoW) corpus was obtained before a term frequency-inverse document frequency (TF-IDF) corpus was derived from it.\n",
    "\n",
    "Topic modeling was then performed on both the BoW corpus (thereafter LDA-BoW) and\n",
    "TF-IDF corpus (thereafter LDA-TFIDF) with the number of topics set to 30, in line with the number of clusters used.\n",
    "\n",
    "The document-topic mapping of each post is then used for computing cosine similarities with all other posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "def get_topics(dictionary, corpus, parsed):\n",
    "    # Train LDA model, get model & topic vectors\n",
    "    # Set training parameters.\n",
    "    num_topics = 30\n",
    "    chunksize = 100\n",
    "    passes = 20\n",
    "    iterations = 400\n",
    "    eval_every = 100  # None = Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make a index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "    \n",
    "    # Get basic evaluation\n",
    "    top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "    # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "    print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "    \n",
    "    # Get topic vectors\n",
    "    all_topics = model.get_document_topics(corpus, per_word_topics=True)\n",
    "    all_topics = [(doc_topics, word_topics, word_phis) for doc_topics, word_topics, word_phis in all_topics]\n",
    "    sen_top = {}\n",
    "    for i in range(len(parsed)):\n",
    "        # These are in the same order as the documents themselves.\n",
    "        doc_topics, word_topics, phi_values = all_topics[i]\n",
    "        # Generate the topic VECTOR not just list of topics\n",
    "        doc_topic_vector = [0] * num_topics\n",
    "        for topic in doc_topics:\n",
    "            doc_topic_vector[topic[0]] = topic[1]\n",
    "        sen_top.update({parsed[i]['post_id']:doc_topic_vector})\n",
    "    \n",
    "    return model, sen_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim import models\n",
    "import pickle\n",
    "\n",
    "def embed_lda(setname):\n",
    "    # Load the parse\n",
    "    with open(f'partials/{setname}_parse.pickle', 'rb') as handle:\n",
    "        parsed = pickle.load(handle)\n",
    "\n",
    "    print(f'embed_lda({setname})')\n",
    "    print('START:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    t0 = time.process_time()\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary([parsed[i]['selftext'] for i in range(len(parsed))])\n",
    "    # print(dictionary)\n",
    "\n",
    "    # Bag-of-words representation of the documents.\n",
    "    corpus = [dictionary.doc2bow(parsed[i]['selftext']) for i in range(len(parsed))]\n",
    "    # for doc in corpus:\n",
    "    #     print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "\n",
    "    # TF-IDF (term freq, inverse document freq) representation\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    # for doc in tfidf[corpus]:\n",
    "    #     print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "\n",
    "    # Get bow data\n",
    "    print(\"Generating topics for BOW...\")\n",
    "    model_bow, sen_top_bow = get_topics(dictionary, corpus, parsed)\n",
    "\n",
    "    # Get tfidf data\n",
    "    print(\"Generating topics for TFIDF...\")\n",
    "    model_tfidf, sen_top_tfidf = get_topics(dictionary, tfidf[corpus], parsed)\n",
    "\n",
    "    print('PROCESS TIME ELAPSED (s)', time.process_time() - t0)\n",
    "    \n",
    "    # Save bow data\n",
    "    with open(f'partials/{setname}_model_top_bow.pickle', 'wb') as handle:\n",
    "        pickle.dump(model_bow, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f'partials/{setname}_embed_top_bow.pickle', 'wb') as handle:\n",
    "        pickle.dump(sen_top_bow, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Save tfidf data\n",
    "    with open(f'partials/{setname}_model_top_tfidf.pickle', 'wb') as handle:\n",
    "        pickle.dump(model_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f'partials/{setname}_embed_top_tfidf.pickle', 'wb') as handle:\n",
    "        pickle.dump(sen_top_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('embed_lda ENDED:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    embed_lda('sample1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Pairwise Cosine Similarity & Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def similarity_clustering(similarity_dict, m, n):\n",
    "    clusters = {};\n",
    "    unselected_posts = similarity_dict.copy()\n",
    "    post_keys = list(unselected_posts.keys())\n",
    "    unselected_keys = list(unselected_posts.keys())\n",
    "    cluster_size = int(np.ceil(n / m))\n",
    "    # print(cluster_size)\n",
    "    while len(unselected_posts) != 0:\n",
    "        selected_post = random.choice(unselected_keys)\n",
    "        # labeling the selected row\n",
    "        emb_dict = dict(zip(post_keys, unselected_posts[selected_post]))\n",
    "        # only sort the unselected columns\n",
    "        sim = {k: emb_dict[k] for k in unselected_keys}\n",
    "        sim_sort = [k for k in sorted(sim.items(), key=lambda item: item[1])][::-1]\n",
    "        cluster_size = int(np.ceil(n / m))\n",
    "        try:\n",
    "            sim_most = sim_sort[0:cluster_size]\n",
    "        except:\n",
    "            sim_most = sim_sort[0:end]\n",
    "        clusters[selected_post] = sim_most\n",
    "        # deleted the selected rows from the unselected\n",
    "        for p in sim_most:\n",
    "            del unselected_posts[p[0]]\n",
    "        unselected_keys = list(unselected_posts.keys())\n",
    "        # print(cluster_size)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def clust_any(setname, embedname, numClusters):\n",
    "    # Load the parse\n",
    "    with open(f'partials/{setname}_parse.pickle', 'rb') as handle:\n",
    "        parsed = pickle.load(handle)\n",
    "    # Load the embed / topic matrix\n",
    "    with open(f'partials/{setname}_embed_{embedname}.pickle', 'rb') as handle:\n",
    "        sen_emb = pickle.load(handle)\n",
    "\n",
    "    print(f'clust_any({setname}, {embedname}, {numClusters})')\n",
    "    print('START:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    print('clustering dataset:', setname, '; embeds:', embedname)\n",
    "    t0 = time.process_time()\n",
    "    numTotalPosts = len(parsed)\n",
    "    d = pd.DataFrame(sen_emb).transpose()\n",
    "    sim_mat = cosine_similarity(d)\n",
    "\n",
    "    post = list(d.index)\n",
    "    post_emb = dict(zip(post, sim_mat))\n",
    "\n",
    "    cluster = similarity_clustering(post_emb, numClusters, numTotalPosts)\n",
    "    # print(cluster)\n",
    "\n",
    "    with open(f'partials/{setname}_clust_{embedname}.pickle', 'wb') as handle:\n",
    "        pickle.dump(cluster, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Transform clusters into a post_id:cluster_id dict\n",
    "    transformed_cluster = {}\n",
    "    clust_num = 0\n",
    "    for key in cluster.keys():\n",
    "        for post in cluster[key]:\n",
    "            transformed_cluster[post[0]] = clust_num\n",
    "        clust_num += 1\n",
    "    # print(transformed_cluster)\n",
    "\n",
    "    print('PROCESS TIME ELAPSED (s)', time.process_time() - t0)\n",
    "    \n",
    "    with open(f'partials/{setname}_clustdict_{embedname}.pickle', 'wb') as handle:\n",
    "        pickle.dump(transformed_cluster, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('clust_any ENDED:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    clust_any('sample1000', 'top_tfidf', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Clusters: Calculate Same-Author-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "\n",
    "def score_sas(setname, embedname):\n",
    "    # Read clusters\n",
    "    with open(f'partials/{setname}_clust_{embedname}.pickle', 'rb') as handle:\n",
    "        clusters = pickle.load(handle)\n",
    "    with open(f'partials/{setname}_clustdict_{embedname}.pickle', 'rb') as handle:\n",
    "        clustdict = pickle.load(handle)\n",
    "    # Read author list\n",
    "    with open(f'partials/{setname}_parse_authors.pickle', 'rb') as handle:\n",
    "        authors = pickle.load(handle)\n",
    "\n",
    "    print(f'score_sas({setname}, {embedname})')\n",
    "    print('START:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    print('scoring dataset:', setname, '; embeds:', embedname)\n",
    "    t0 = time.process_time()\n",
    "    \n",
    "    num_clust_pair = 0\n",
    "    num_total_pair = 0\n",
    "\n",
    "    for auth in authors:\n",
    "        # authors[auth] is a list of post IDs made by author 'auth'\n",
    "        if len(authors[auth]) < 2:\n",
    "            continue\n",
    "        for pair in itertools.product(authors[auth],authors[auth]):\n",
    "            if pair[0] == pair[1]:\n",
    "                continue\n",
    "            num_total_pair += 1\n",
    "            if clustdict[pair[0]] == clustdict[pair[1]]:\n",
    "                num_clust_pair += 1\n",
    "\n",
    "    score_sas = (num_clust_pair / num_total_pair) - (1/len(clusters))\n",
    "    print('PROCESS TIME ELAPSED (s)', time.process_time() - t0)\n",
    "    print('score_sas ENDED:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    return score_sas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    a = score_sas('sample1000', 'top_tfidf')\n",
    "    # TODO: this seems to be 10x larger than what the paper reports for SAS score ...\n",
    "    # Is this because of the dataset size?\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Clusters: Calculate Jaccard Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "def score_jaccard(setname, embedname):\n",
    "    # Read post data\n",
    "    with open(f'partials/{setname}_parse.pickle', 'rb') as handle:\n",
    "        parsed = pickle.load(handle)\n",
    "    # Read clusters\n",
    "    with open(f'partials/{setname}_clust_{embedname}.pickle', 'rb') as handle:\n",
    "        clusters = pickle.load(handle)\n",
    "    with open(f'partials/{setname}_clustdict_{embedname}.pickle', 'rb') as handle:\n",
    "        clustdict = pickle.load(handle)\n",
    "    # Read author list\n",
    "    with open(f'partials/{setname}_parse_authors.pickle', 'rb') as handle:\n",
    "        authors = pickle.load(handle)\n",
    "    # Read author subreddits\n",
    "    with open(f'data/authorsubs.json', 'r') as fp:\n",
    "        sub_mappings = json.load(fp)\n",
    "\n",
    "    print(f'score_jaccard({setname}, {embedname})')\n",
    "    print('START:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    print('scoring dataset:', setname, '; embeds:', embedname)\n",
    "    t0 = time.process_time()\n",
    "    \n",
    "    # Transform parsed into something more usable for Jaccard\n",
    "    metadata = {}\n",
    "    for p in parsed:\n",
    "        metadata[p['post_id']] = p\n",
    "\n",
    "    # Set up constants\n",
    "    target_sub = 'Advice'\n",
    "    default_subs = {\n",
    "        'comment': [target_sub],\n",
    "        'submission': [target_sub]\n",
    "    }\n",
    "\n",
    "    intersect_sum = 0\n",
    "    for clustkey in clusters.keys():\n",
    "        ids_in_clust = [i[0] for i in clusters[clustkey]]\n",
    "        for pair in itertools.product(ids_in_clust,ids_in_clust):\n",
    "            a0 = metadata[pair[0]]['author']\n",
    "            a1 = metadata[pair[1]]['author']\n",
    "            a0_subs = sub_mappings[a0] if (a0 in sub_mappings) else default_subs\n",
    "            a1_subs = sub_mappings[a1] if (a1 in sub_mappings) else default_subs\n",
    "            # Check for \"throwaways\"\n",
    "            a0_subs_total = set(a0_subs['comment'] + a0_subs['submission'])\n",
    "            a1_subs_total = set(a1_subs['comment'] + a1_subs['submission'])\n",
    "            if len(a0_subs_total) == 1:\n",
    "                continue\n",
    "            if len(a1_subs_total) == 1:\n",
    "                continue\n",
    "            # New formulation: use set of subreddits an author has ever interacted with\n",
    "            intersect_sum += (\n",
    "                len(a0_subs_total.intersection(a1_subs_total))\n",
    "                /\n",
    "                len(a0_subs_total.union(a1_subs_total))\n",
    "            )\n",
    "            # Original paper formulation: this fails if neither author has ever commented on a sub\n",
    "#             comment_subscore = (\n",
    "#                 len(set(a0_subs['comment']).intersection(set(a1_subs['comment'])))\n",
    "#                 /\n",
    "#                 len(set(a0_subs['comment']).union(set(a1_subs['comment'])))\n",
    "#             )\n",
    "#             submits_subscore = (\n",
    "#                 len(set(a0_subs['submission']).intersection(set(a1_subs['submission'])))\n",
    "#                 /\n",
    "#                 len(set(a0_subs['submission']).union(set(a1_subs['submission'])))\n",
    "#             )\n",
    "#             intersect_sum += 0.5 * (comment_subscore + submits_subscore)\n",
    "    score_jaccard = intersect_sum * len(clusters) / (len(clustdict) ** 2)\n",
    "    print('PROCESS TIME ELAPSED (s)', time.process_time() - t0)\n",
    "    print('score_jaccard ENDED:', time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()))\n",
    "    return score_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    a = score_jaccard('sample1000', 'top_bow')\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now run the entire reference pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Load constants / large loading items\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Parameters\n",
    "setname = 'sample1000'\n",
    "src_file = 'data/final_proj_data_preprocessed_1000sample.csv'\n",
    "num_clusters = 6\n",
    "\n",
    "# Load basic data\n",
    "parse_reddit_csv(src_file, setname, \n",
    "                stop_words=stop_words, lemmatizer=lemmatizer, tokenizer=tokenizer)\n",
    "\n",
    "# Generate embeddings for reference models\n",
    "embed_w2v(setname, model=model)\n",
    "embed_lda(setname)\n",
    "\n",
    "# Cluster and score in loops\n",
    "embed_types = ['top_tfidf', 'top_bow', 'w2v_weighted', 's2v_sif']\n",
    "scores = {\n",
    "    'sas': {\n",
    "        'top_tfidf': [],\n",
    "        'top_bow': [],\n",
    "        'w2v_weighted': [],\n",
    "        's2v_sif': [],\n",
    "    },\n",
    "    'jaccard': {\n",
    "        'top_tfidf': [],\n",
    "        'top_bow': [],\n",
    "        'w2v_weighted': [],\n",
    "        's2v_sif': [],\n",
    "    }\n",
    "}\n",
    "for i in range(100):\n",
    "    for embed_name in embed_types:\n",
    "        clust_any(setname, embed_name, num_clusters)\n",
    "    for embed_name in embed_types:\n",
    "        scores['sas'][embed_name].append(score_sas(setname, embed_name))\n",
    "        scores['jaccard'][embed_name].append(score_jaccard(setname, embed_name))\n",
    "\n",
    "# Save scores\n",
    "time_string = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
    "with open(f'outputs/{setname}_scores_{time_string}.pickle', 'wb') as handle:\n",
    "    pickle.dump(transformed_cluster, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
